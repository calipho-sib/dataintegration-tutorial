# Building Scalable Data Integration Pipelines Using Container Technology

In order to accommodate big volumes of data, scalable data integration pipelines need to be developed. The aim of this tutorial is to present the steps to build a scalable data integration pipeline using container technology and state-of-the-art workflow management tools.
This tutorial walks you through on how to use docker and apache airflow to build a simple data integration pipeline.

This tutorial is organized in three sessions;

1. Introduction  to Containers, Docker and Docker Compose
2. Intrdocution to Apache Airflow
3. Data Integration with Docker and Airflow


#### Installation
```
Give examples
```

#### Running a docker container

### Container Orchestration with Docker Swarm

#### Why Orchestration

```
Give the example
```

And repeat

```
until finished
```

End with an example of getting some data out of the system or using it for a little demo


### Example with Docker and Docker Swarm Operator 


