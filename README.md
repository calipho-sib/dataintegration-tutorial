# Building Scalable Data Integration Pipelines Using Container Technology

In order to accommodate big volumes of data, scalable data integration pipelines need to be developed. The aim of this tutorial is to present the steps to build a scalable data integration pipeline using container technology and state-of-the-art workflow management tools.
This tutorial walks you through on how to use docker and apache airflow to build a simple data integration pipeline.

This tutorial is organized in three sessions;

1. Introduction  to Containers, Docker and Docker Compose
2. Introduction to Apache Airflow
3. Data Integration with Docker and Airflow

README file in different directories under **/sessions** describes how to follow each session. Along with the README files other resources/files are also included in those directories.
