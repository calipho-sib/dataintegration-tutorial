# Cloud Native Scalable Data Integration Pipeline

In order to accommodate large scale data, scalable data integration pipelines need to be developed. The aim of this tutorial is to present the steps to build a scalable data integration pipeline using container technology and state-of-the-art workflow management tools.

## Container Technology with Docker


### Docker Basics


#### Installation
```
Give examples
```

#### Running a docker container

### Container Orchestration with Docker Swarm

#### Why Orchestration

```
Give the example
```

And repeat

```
until finished
```

End with an example of getting some data out of the system or using it for a little demo

## Workflow Management with Apache Airflow

Explain how to run the automated tests for this system

### Airflow Basics

Explain what these tests test and why

```
Give an example
```

### Airflow Installation 

Explain what these tests test and why

```
Give an example
```

### Airflow Basics

Add additional notes about how to deploy this on a live system

### First example with Airflow

### Example with Docker and Docker Swarm Operator 


## Data Integration Pipeline Patterns

### Synchronous I/O based pipelines  

### Producer consumer based pipelines